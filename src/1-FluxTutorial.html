<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-07-23 Mon 11:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Flux tutorial</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Sergio-Feliciano Mendoza-Barrera" />
<meta name="description" content="Some business ideas that can be implemented in Canada in short term"
 />
<meta name="keywords" content="R, data science, emacs, ESS, org-mode, computer vision, hardware" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Flux tutorial</h1>
<div class="ABSTRACT">
<p>
<b>Flux: The Julia Machine Learning Library</b>
</p>

<p>
Flux is a library for machine learning. It comes "batteries-included"
with many useful tools built in, but also lets you use the full power
of the Julia language where you need it. The whole stack is
implemented in clean Julia code (right down to the <a href="https://github.com/FluxML/CuArrays.jl">GPU kernels</a>) and
any part can be tweaked to your liking.
</p>

<p>
The project <a href="http://fluxml.ai/Flux.jl">URL</a>.
</p>

</div>

<div id="outline-container-orga71463d" class="outline-2">
<h2 id="orga71463d"><span class="section-number-2">1</span> Installation</h2>
<div class="outline-text-2" id="text-1">
<p>
Install Julia 0.6.0 or later, if you haven't already.
</p>

<div class="org-src-container">
<pre class="src src-text">Pkg.add("Flux")
# Optional but recommended
Pkg.update() # Keep your packages up to date
Pkg.test("Flux") # Check things installed correctly
</pre>
</div>

<p>
Start with the basics. The model zoo is also a good starting point for
many common kinds of models.
</p>

<div class="org-src-container">
<pre class="src src-julia">println(<span style="color: #5f9411; background-color: #eff8e9;">":: Julia initialization! ::"</span>)
Pkg.update()
</pre>
</div>

<pre class="example">
:: Julia initialization! ::
INFO: Updating METADATA...
INFO: Computing changes...
INFO: No packages to install, update or remove

</pre>

<p>
See GPU support for more details on installing and using Flux with
GPUs.
</p>
</div>
</div>

<div id="outline-container-org04e1c7a" class="outline-2">
<h2 id="org04e1c7a"><span class="section-number-2">2</span> Model-Building Basics</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org2e83204" class="outline-3">
<h3 id="org2e83204"><span class="section-number-3">2.1</span> Taking Gradients</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Consider a simple linear regression, which tries to predict an output
array y from an input x. (It's a good idea to follow this example in
the Julia repl.)
</p>

<div class="org-src-container">
<pre class="src src-julia">W = rand(2, 5)
b = rand(2)

<span style="color: #d15120; background-color: #fdf2ed;">predict</span>(x) = W*x .+ b
<span style="color: #d15120; background-color: #fdf2ed;">loss</span>(x, y) = sum((predict(x) .- y).^2)

x, y = rand(5), rand(2) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">Dummy data</span>
loss(x, y) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">~ 3</span>
</pre>
</div>

<pre class="example">
2×5 Array{Float64,2}:
 0.174258  0.750107  0.319624  0.145033  0.735474
 0.664298  0.932769  0.214092  0.664086  0.797072
2-element Array{Float64,1}:
 0.390068
 0.815136

predict (generic function with 1 method)
loss (generic function with 1 method)

([0.224385, 0.338351, 0.279211, 0.54581, 0.833118], [0.538516, 0.216036])
5.479478967184904
</pre>

<p>
<b>Addendum</b>: Using fixed data.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> DataFrames;
<span style="color: #cf7900; background-color: #fdf9f2;">using</span> CSV;
data = CSV.read(<span style="color: #5f9411; background-color: #eff8e9;">"./data.csv"</span>)
size(data)
names(data)
</pre>
</div>

<pre class="example">
INFO: Recompiling stale cache file /home/sergio/.julia/lib/v0.6/CodecZlib.ji for module CodecZlib.
INFO: Recompiling stale cache file /home/sergio/.julia/lib/v0.6/DataFrames.ji for module DataFrames.
INFO: Recompiling stale cache file /home/sergio/.julia/lib/v0.6/CSV.ji for module CSV.
23×2 DataFrames.DataFrame
│ Row │ X    │ Y    │
├─────┼──────┼──────┤
│ 1   │ 12.4 │ 11.2 │
│ 2   │ 14.3 │ 12.5 │
│ 3   │ 14.5 │ 12.7 │
│ 4   │ 14.9 │ 13.1 │
│ 5   │ 16.1 │ 14.1 │
│ 6   │ 16.9 │ 14.8 │
│ 7   │ 16.5 │ 14.4 │
│ 8   │ 15.4 │ 13.4 │
⋮
│ 15  │ 15.5 │ 14.0 │
│ 16  │ 16.7 │ 14.6 │
│ 17  │ 17.3 │ 15.1 │
│ 18  │ 18.4 │ 16.1 │
│ 19  │ 19.2 │ 16.8 │
│ 20  │ 17.4 │ 15.2 │
│ 21  │ 19.5 │ 17.0 │
│ 22  │ 19.7 │ 17.2 │
│ 23  │ 21.2 │ 18.6 │
(23, 2)
2-element Array{Symbol,1}:
 :X
 :Y
</pre>

<p>
Now lets replicate the previous regression steps.
</p>

<div class="org-src-container">
<pre class="src src-julia">W = rand(2, 5)
b = rand(2)

<span style="color: #d15120; background-color: #fdf2ed;">predict</span>(x) = W*x .+ b
<span style="color: #d15120; background-color: #fdf2ed;">loss</span>(x, y) = sum((predict(x) .- y).^2)

x, y = data[1:5, <span style="color: #cf7900; background-color: #fdf9f2;">:X</span>], data[1:2, <span style="color: #cf7900; background-color: #fdf9f2;">:Y</span>] <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">Dummy data</span>
loss(x, y) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">~ 3</span>
</pre>
</div>

<pre class="example">
2×5 Array{Float64,2}:
 0.321359  0.254585  0.312787  0.94669    0.551409
 0.475509  0.403494  0.489568  0.0585563  0.435709
2-element Array{Float64,1}:
 0.370281
 0.907944

predict (generic function with 1 method)
loss (generic function with 1 method)

(Union{Float64, Missings.Missing}[12.4, 14.3, 14.5, 14.9, 16.1], Union{Float64, Missings.Missing}[11.2, 12.5])
818.0080815776166
</pre>

<p>
To improve the prediction we can take the gradients of W and b with
respect to the loss function and perform gradient descent. We could
calculate gradients by hand, but Flux will do it for us if we tell it
that W and b are trainable <i>parameters</i>.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux.Tracker

W = param(W)
b = param(b)

l = loss(x, y)

back!(l)
</pre>
</div>

<pre class="example">


Tracked 2×5 Array{Float64,2}:
 0.321359  0.254585  0.312787  0.94669    0.551409
 0.475509  0.403494  0.489568  0.0585563  0.435709
Tracked 2-element Array{Float64,1}:
 0.370281
 0.907944

818.0080815776166 (tracked)

</pre>

<p>
<code>loss(x, y)</code> returns the same number, but it's now a <i>tracked</i> value
that records gradients as it goes along. Calling <code>back!</code> then
accumulates the gradient of <code>W</code> and <code>b</code>. We can see what this gradient
is, and modify <code>W</code> to train the model.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux.Tracker: grad, update!

&#916; = grad(W)

<span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">Update the parameter and reset the gradient</span>
update!(W, -0.1&#916;)

loss(x, y) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">~ 2.5</span>
</pre>
</div>

<pre class="example">


2×5 Array{Float64,2}:
 602.999  695.394  705.12  724.571  782.926
 373.497  430.726  436.75  448.798  484.943


Tracked 2×5 Array{Float64,2}:
 -59.9785  -69.2848  -70.1992  -71.5105  -77.7412
 -36.8742  -42.6691  -43.1854  -44.8213  -48.0586

3.571226543305759e7 (tracked)
</pre>

<p>
The loss has decreased a little, meaning that our prediction <code>x</code> is
closer to the target <code>y</code>. If we have some data we can already try
training the model.
</p>


<div id="orge624fe9" class="figure">
<p><img src="../graphs/regression001.png" alt="regression001.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Simple linear regression</p>
</div>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> GLM
ols = lm(<span style="color: #cf7900; background-color: #fdf9f2;">@formula</span>(Y ~ X), data)
GLM.stderror(ols)
GLM.predict(ols)
</pre>
</div>

<pre class="example">

StatsModels.DataFrameRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}

Formula: Y ~ 1 + X

Coefficients:
             Estimate Std.Error t value Pr(&gt;|t|)
(Intercept)  0.434584  0.177049 2.45461   0.0229
X            0.851144 0.0100458 84.7259   &lt;1e-27
2-element Array{Float64,1}:
 0.177049
 0.0100458
23-element Array{Float64,1}:
 10.9888
 12.6059
 12.7762
 13.1166
 14.138
 14.8189
 14.4785
 13.5422
 14.904
 15.6701
  ⋮
 13.6273
 14.6487
 15.1594
 16.0956
 16.7766
 15.2445
 17.0319
 17.2021
 18.4788
</pre>

<p>
All deep learning in Flux, however complex, is a simple generalisation
of this example. Of course, models can look very different – they
might have millions of parameters or complex control flow, and there
are ways to manage this complexity. Let's see what that looks like.
</p>
</div>
</div>

<div id="outline-container-org27de658" class="outline-3">
<h3 id="org27de658"><span class="section-number-3">2.2</span> Building Layers</h3>
<div class="outline-text-3" id="text-2-2">
<p>
It's common to create more complex models than the linear regression
above. For example, we might want to have two linear layers with a
nonlinearity like <code>sigmoid</code> (<code>σ</code>) in between them. In the above style we
could write this as:
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux

W1 = param(rand(3, 5))
b1 = param(rand(3))
<span style="color: #d15120; background-color: #fdf2ed;">layer1</span>(x) = W1 * x .+ b1

W2 = param(rand(2, 3))
b2 = param(rand(2))
<span style="color: #d15120; background-color: #fdf2ed;">layer2</span>(x) = W2 * x .+ b2

<span style="color: #d15120; background-color: #fdf2ed;">model</span>(x) = layer2(&#963;.(layer1(x)))

model(rand(5)) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 2-element vector</span>
</pre>
</div>

<pre class="example">


Tracked 3×5 Array{Float64,2}:
 0.411888  0.875844  0.185673   0.210546  0.121761
 0.391358  0.587994  0.041861   0.266823  0.203698
 0.825134  0.16344   0.0779545  0.921908  0.974163
Tracked 3-element Array{Float64,1}:
 0.203049
 0.582317
 0.0349656
layer1 (generic function with 1 method)

Tracked 2×3 Array{Float64,2}:
 0.646053  0.981485  0.838297
 0.493253  0.245144  0.446347
Tracked 2-element Array{Float64,1}:
 0.942242
 0.894073
layer2 (generic function with 1 method)

model (generic function with 1 method)

Tracked 2-element Array{Float64,1}:
 2.74306
 1.75006
</pre>

<p>
This works but is fairly unwieldy, with a lot of repetition –
especially as we add more layers. One way to factor this out is to
create a function that returns linear layers.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">function</span> <span style="color: #d15120; background-color: #fdf2ed;">linear</span>(<span style="color: #cf7900; background-color: #fdf9f2;">in</span>, out)
    W = param(randn(out, <span style="color: #cf7900; background-color: #fdf9f2;">in</span>))
    b = param(randn(out))
    x -&gt; W * x .+ b
<span style="color: #cf7900; background-color: #fdf9f2;">end</span>

linear1 = linear(5, 3) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">we can access linear1.W etc</span>
linear2 = linear(3, 2)

<span style="color: #d15120; background-color: #fdf2ed;">model</span>(x) = linear2(&#963;.(linear1(x)))

model(x) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 2-element vector</span>
</pre>
</div>

<pre class="example">
linear (generic function with 1 method)

(::#3) (generic function with 1 method)
(::#3) (generic function with 1 method)

model (generic function with 1 method)

Tracked 2-element Array{Float64,1}:
 -1.91996
 -1.19972
</pre>

<p>
Another (equivalent) way is to create a struct that explicitly
represents the affine layer.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">struct</span> <span style="color: #b23f1e; background-color: #fcf3f1;">Affine</span>
    W
    b
<span style="color: #cf7900; background-color: #fdf9f2;">end</span>

<span style="color: #d15120; background-color: #fdf2ed;">Affine</span>(<span style="color: #cf7900; background-color: #fdf9f2;">in</span><span style="color: #505050; background-color: #FFFFFF;">::</span><span style="color: #b23f1e; background-color: #fcf3f1;">Integer</span>, out<span style="color: #505050; background-color: #FFFFFF;">::</span><span style="color: #b23f1e; background-color: #fcf3f1;">Integer</span>) =
    Affine(param(randn(out, <span style="color: #cf7900; background-color: #fdf9f2;">in</span>)), param(randn(out)))

<span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">Overload call, so the object can be used as a function</span>
(m<span style="color: #505050; background-color: #FFFFFF;">::</span><span style="color: #b23f1e; background-color: #fcf3f1;">Affine</span>)(x) = m.W * x .+ m.b

a = Affine(10, 5)

a(rand(10)) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 5-element vector</span>
</pre>
</div>

<pre class="example">


Affine




Affine(param([-1.6488 0.376519 … 0.941953 -0.363354; 0.569429 -0.740986 … 1.19353 -0.420238; … ; -0.476403 -1.15401 … -1.82431 0.893862; 2.32319 -1.16424 … -1.01693 -0.51625]), param([-0.467816, -0.854028, -0.959012, 0.468946, 2.09242]))

Tracked 5-element Array{Float64,1}:
  0.294692
 -1.66658
 -3.77026
 -0.706733
 -0.238175
</pre>

<p>
Congratulations! You just built the Dense layer that comes with
Flux. Flux has many interesting layers available, but they're all
things you could have built yourself very easily.
</p>

<p>
(There is one small difference with <code>Dense</code> – for convenience it also
takes an activation function, like <code>Dense(10, 5, σ)</code>.)
</p>
</div>
</div>

<div id="outline-container-orgc08c74f" class="outline-3">
<h3 id="orgc08c74f"><span class="section-number-3">2.3</span> Stacking It Up</h3>
<div class="outline-text-3" id="text-2-3">
<p>
It's pretty common to write models that look something like:
</p>

<div class="org-src-container">
<pre class="src src-text">layer1 = Dense(10, 5, &#963;)
# ...
model(x) = layer3(layer2(layer1(x)))
</pre>
</div>

<p>
For long chains, it might be a bit more intuitive to have a list of
layers, like this:
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux

layers = [Dense(10, 5, &#963;), Dense(5, 2), softmax]
<span style="color: #d15120; background-color: #fdf2ed;">model</span>(x) = foldl((x, m) -&gt; m(x), x, layers)
model(rand(10)) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 2-element vector</span>
</pre>
</div>

<pre class="example">


3-element Array{Any,1}:
 Dense(10, 5, NNlib.σ)
 Dense(5, 2)
 NNlib.softmax
model (generic function with 1 method)
Tracked 2-element Array{Float64,1}:
 0.849323
 0.150677
</pre>

<p>
Handily, this is also provided for in Flux:
</p>

<div class="org-src-container">
<pre class="src src-julia">model2 = Chain(
    Dense(10, 5, &#963;),
    Dense(5, 2),
    softmax
)

model2(rand(10)) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 2-element vector</span>
</pre>
</div>

<pre class="example">
Chain(Dense(10, 5, NNlib.σ), Dense(5, 2), NNlib.softmax)

Tracked 2-element Array{Float64,1}:
 0.522415
 0.477585

</pre>

<p>
This quickly starts to look like a high-level deep learning library;
yet you can see how it falls out of simple abstractions, and we lose
none of the power of Julia code.
</p>

<p>
A nice property of this approach is that because "models" are just
functions (possibly with trainable parameters), you can also see this
as simple function composition.
</p>

<div class="org-src-container">
<pre class="src src-julia">m = Dense(5, 2) &#8728; Dense(10, 5, &#963;)

m(rand(10))
</pre>
</div>

<pre class="example">
(::#55) (generic function with 1 method)

Tracked 2-element Array{Float64,1}:
 -0.162062
 -0.279983

</pre>

<p>
Likewise, <code>Chain</code> will happily work with any Julia function.
</p>

<div class="org-src-container">
<pre class="src src-julia">m = Chain(x -&gt; x^2, x -&gt; x+1)

m(5) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">=&gt; 26</span>
</pre>
</div>

<pre class="example">
Chain(#7, #8)

26

</pre>
</div>
</div>

<div id="outline-container-org4ce347a" class="outline-3">
<h3 id="org4ce347a"><span class="section-number-3">2.4</span> Layer helpers</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Flux provides a set of helpers for custom layers, which you can enable
by calling
</p>

<div class="org-src-container">
<pre class="src src-julia">Flux.treelike(Affine)
</pre>
</div>

<p>
This enables a useful extra set of functionality for our Affine layer,
such as collecting its parameters or <a href="http://fluxml.ai/Flux.jl/stable/gpu.html">moving it to the GPU</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-orga2d7b98" class="outline-2">
<h2 id="orga2d7b98"><span class="section-number-2">3</span> Recurrent Models</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org58c5de2" class="outline-3">
<h3 id="org58c5de2"><span class="section-number-3">3.1</span> Recurrent Cells</h3>
<div class="outline-text-3" id="text-3-1">
<p>
In the simple feedforward case, our model m is a simple function from
various inputs xᵢ to predictions yᵢ. (For example, each x might be an
MNIST digit and each y a digit label.) Each prediction is completely
independent of any others, and using the same x will always produce
the same y.
</p>

<div class="org-src-container">
<pre class="src src-text">y&#8321; = f(x&#8321;)
y&#8322; = f(x&#8322;)
y&#8323; = f(x&#8323;)
# ...
</pre>
</div>

<p>
Recurrent networks introduce a hidden state that gets carried over
each time we run the model. The model now takes the old h as an input,
and produces a new h as output, each time we run it.
</p>

<div class="org-src-container">
<pre class="src src-text">h = # ... initial state ...
h, y&#8321; = f(h, x&#8321;)
h, y&#8322; = f(h, x&#8322;)
h, y&#8323; = f(h, x&#8323;)
# ...
</pre>
</div>

<p>
Information stored in <code>h</code> is preserved for the next prediction, allowing
it to function as a kind of memory. This also means that the
prediction made for a given x depends on all the inputs previously fed
into the model.
</p>

<p>
(This might be important if, for example, each x represents one word
of a sentence; the model's interpretation of the word "bank" should
change if the previous input was "river" rather than "investment".)
</p>

<p>
Flux's RNN support closely follows this mathematical perspective. The
most basic RNN is as close as possible to a standard <code>Dense</code> layer,
and the output is also the hidden state.
</p>

<div class="org-src-container">
<pre class="src src-julia">Wxh = randn(5, 10)
Whh = randn(5, 5)
b   = randn(5)

<span style="color: #cf7900; background-color: #fdf9f2;">function</span> <span style="color: #d15120; background-color: #fdf2ed;">rnn</span>(h, x)
    h = tanh.(Wxh * x .+ Whh * h .+ b)
    <span style="color: #cf7900; background-color: #fdf9f2;">return</span> h, h
<span style="color: #cf7900; background-color: #fdf9f2;">end</span>

x = rand(10) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">dummy data</span>
h = rand(5)  <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">initial hidden state</span>

h, y = rnn(h, x)
</pre>
</div>

<pre class="example">
5×10 Array{Float64,2}:
 -0.335602  -0.743067   1.56416    -0.745028  -1.31895     0.828944   0.307544  -2.13057    0.565697   1.62416
 -0.296219  -0.628833   0.567573   -0.60324   -0.0924562   1.81886   -0.29517   -1.06414   -0.417291   0.0375811
  0.311554  -1.3975     0.0653115  -1.36585   -0.0732771   0.307589  -1.01955    0.724853  -1.22862    1.11857
 -0.22783   -0.330792   0.822159    1.80235    0.177961    1.56767    1.43587   -0.448002   0.277141   0.840428
 -0.25141    1.55337   -1.85621     0.817607  -0.822015   -2.55036    0.620046   2.00377    1.4629    -1.44022
5×5 Array{Float64,2}:
  0.161084    1.58312   -1.47054    0.0423842  -1.11387
 -0.0640146   1.84554    0.148866   1.02162     0.197515
  0.100668    0.184159  -0.86423   -0.156539   -0.176746
 -0.965176   -1.3083     0.81521   -0.0504029  -0.546809
  0.716647   -0.711621  -1.4207     1.78595    -1.81752
5-element Array{Float64,1}:
  1.55872
 -0.0863359
  0.292448
 -0.215982
  1.09435

rnn (generic function with 1 method)

10-element Array{Float64,1}:
 0.704938
 0.643769
 0.416595
 0.70418
 0.783186
 0.817874
 0.853267
 0.598577
 0.831481
 0.721251
5-element Array{Float64,1}:
 0.077753
 0.624197
 0.9162
 0.798651
 0.761921

([0.083027, 0.916704, -0.991142, 0.99861, -0.636675], [0.083027, 0.916704, -0.991142, 0.99861, -0.636675])
</pre>

<p>
If you run the last line a few times, you'll notice the output y
changing slightly even though the input <code>x</code> is the same.
</p>

<p>
We sometimes refer to functions like rnn above, which explicitly
manage state, as recurrent cells. There are various recurrent cells
available, which are documented in the <code>layer reference</code>. The
hand-written example above can be replaced with:
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux

rnn2 = Flux.RNNCell(10, 5)

x = rand(10) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">dummy data</span>
h = rand(5)  <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">initial hidden state</span>

h, y = rnn2(h, x)
</pre>
</div>

<pre class="example">


RNNCell(10, 5, tanh)

10-element Array{Float64,1}:
 0.77719
 0.5867
 0.918167
 0.771562
 0.198712
 0.726097
 0.77191
 0.868939
 0.880878
 0.818349
5-element Array{Float64,1}:
 0.39893
 0.895915
 0.21959
 0.138349
 0.219127

(param([-0.595215, 0.815866, -0.927057, 0.854788, 0.223998]), param([-0.595215, 0.815866, -0.927057, 0.854788, 0.223998]))
</pre>
</div>
</div>

<div id="outline-container-orgf9bdbc4" class="outline-3">
<h3 id="orgf9bdbc4"><span class="section-number-3">3.2</span> Stateful Models</h3>
<div class="outline-text-3" id="text-3-2">
<p>
For the most part, we don't want to manage hidden states ourselves,
but to treat our models as being stateful. Flux provides the <code>Recur</code>
wrapper to do this.
</p>

<div class="org-src-container">
<pre class="src src-julia">x = rand(10)
h = rand(5)

m = Flux.Recur(rnn, h)

y = m(x)
</pre>
</div>

<pre class="example">
10-element Array{Float64,1}:
 0.864184
 0.727019
 0.152897
 0.596222
 0.696429
 0.795441
 0.970867
 0.756388
 0.309554
 0.270187
5-element Array{Float64,1}:
 0.286996
 0.820282
 0.469629
 0.730869
 0.669104

Recur(rnn)

5-element Array{Float64,1}:
 -0.458061
  0.91275
 -0.966963
  0.957575
  0.682724
</pre>

<p>
The <code>Recur</code> wrapper stores the state between runs in the <code>m.state</code>
field.
</p>

<p>
If you use the <code>RNN(10, 5)</code> constructor – as opposed to <code>RNNCell</code> –
you'll see that it's simply a wrapped cell.
</p>

<div class="org-src-container">
<pre class="src src-julia">RNN(10, 5)
</pre>
</div>

<pre class="example">
Recur(RNNCell(10, 5, tanh))

</pre>
</div>
</div>

<div id="outline-container-org97d6549" class="outline-3">
<h3 id="org97d6549"><span class="section-number-3">3.3</span> Sequences</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Often we want to work with sequences of inputs, rather than individual
<code>x</code>.
</p>

<div class="org-src-container">
<pre class="src src-julia">seq = [rand(10) <span style="color: #cf7900; background-color: #fdf9f2;">for</span> i = 1:10]
</pre>
</div>

<pre class="example">
10-element Array{Array{Float64,1},1}:
 [0.26384, 0.511612, 0.410464, 0.628855, 0.400572, 0.356883, 0.650063, 0.832428, 0.803437, 0.619356]
 [0.0558729, 0.55321, 0.111685, 0.847347, 0.231906, 0.421247, 0.535692, 0.967631, 0.151788, 0.543204]
 [0.563864, 0.0166117, 0.155863, 0.397863, 0.871601, 0.558577, 0.12065, 0.292529, 0.0516992, 0.0147827]
 [0.732378, 0.886645, 0.619666, 0.229015, 0.34623, 0.620486, 0.263806, 0.409757, 0.941559, 0.42633]
 [0.629197, 0.879354, 0.690331, 0.932031, 0.828211, 0.0850166, 0.216663, 0.973238, 0.850589, 0.792926]
 [0.279912, 0.770641, 0.782412, 0.279212, 0.455268, 0.0319396, 0.636586, 0.0350002, 0.670798, 0.592461]
 [0.745074, 0.189212, 0.722463, 0.491579, 0.893423, 0.958347, 0.405673, 0.81953, 0.313067, 0.754308]
 [0.15898, 0.204461, 0.0246603, 0.368311, 0.514293, 0.081758, 0.611936, 0.861855, 0.604147, 0.651089]
 [0.0211448, 0.586099, 0.20338, 0.971617, 0.80417, 0.986488, 0.407577, 0.26519, 0.467627, 0.643396]
 [0.50283, 0.0831621, 0.403768, 0.362624, 0.00741494, 0.173167, 0.837751, 0.335071, 0.964773, 0.753252]
</pre>

<p>
With <code>Recur</code>, applying our model to each element of a sequence is
trivial:
</p>

<div class="org-src-container">
<pre class="src src-julia">m.(seq) <span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;"># </span><span style="color: #a49da5; background-color: #f7f7f7; font-style: italic;">returns a list of 5-element vectors</span>
</pre>
</div>

<pre class="example">
10-element Array{Array{Float64,1},1}:
 [0.994967, 0.85456, -0.640273, 0.760783, 0.998368]
 [0.821612, 0.764942, -0.387668, -0.414733, 0.997903]
 [0.701053, 0.841691, 0.469026, -0.777716, -0.982692]
 [0.998918, 0.14712, -0.876748, 0.855405, 0.685643]
 [0.650053, -0.850078, -0.780308, 0.350226, 0.99994]
 [0.922316, -0.965164, -0.902312, 0.92306, 0.971687]
 [0.228973, -0.526887, 0.70814, 0.990793, 0.696663]
 [-0.980654, -0.890894, -0.892708, 0.983864, 0.995825]
 [0.0141673, -0.289343, -0.903104, 0.999929, 0.72949]
 [0.996927, -0.417548, -0.532885, 0.966277, 0.99871]
</pre>

<p>
This works even when we've chain recurrent layers into a larger
model.
</p>

<div class="org-src-container">
<pre class="src src-julia">m = Chain(LSTM(10, 15), Dense(15, 5))
m.(seq)
</pre>
</div>

<pre class="example">
Chain(Recur(LSTMCell(10, 15)), Dense(15, 5))
10-element Array{TrackedArray{…,Array{Float64,1}},1}:
 param([-0.0345502, -0.0403766, 0.00558828, 0.0442483, -0.0169786])
 param([-0.0675349, 0.00197485, -0.0297345, 0.02987, -0.0444642])
 param([-0.0693338, 0.0209713, -0.0220656, 0.0911402, -0.00945156])
 param([-0.0801739, -0.0261297, -0.00825265, 0.204538, -0.0212383])
 param([-0.110197, -0.0649395, -0.0246703, 0.196527, -0.0645964])
 param([-0.0480114, -0.0303197, -0.0907227, 0.216228, -0.0192784])
 param([-0.104461, -0.0425815, -0.139016, 0.268567, -0.0400302])
 param([-0.0813874, -0.135401, -0.0955492, 0.230069, -0.000189431])
 param([-0.0576492, -0.0359034, -0.18636, 0.2428, -0.0569451])
 param([0.0116163, -0.0660804, -0.1669, 0.239081, 0.0383325])
</pre>
</div>
</div>

<div id="outline-container-orgf5af412" class="outline-3">
<h3 id="orgf5af412"><span class="section-number-3">3.4</span> Truncating Gradients</h3>
<div class="outline-text-3" id="text-3-4">
<p>
By default, calculating the gradients in a recurrent layer involves
the entire history. For example, if we call the model on 100 inputs,
calling <code>back!</code> will calculate the gradient for those 100 calls. If we
then calculate another 10 inputs we have to calculate 110 gradients –
this accumulates and quickly becomes expensive.
</p>

<p>
To avoid this we can truncate the gradient calculation, forgetting the
history.
</p>

<div class="org-src-container">
<pre class="src src-julia">Flux.truncate!(m)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-julia">m
</pre>
</div>

<pre class="example">
Chain(Recur(LSTMCell(10, 15)), Dense(15, 5))

</pre>

<p>
Calling <code>truncate!</code> wipes the slate clean, so we can call the model
with more inputs without building up an expensive gradient
computation.
</p>

<p>
<code>truncate!</code> makes sense when you are working with multiple chunks of a
large sequence, but we may also want to work with a set of independent
sequences. In this case the hidden state should be completely reset to
its original value, throwing away any accumulated information. <code>reset!</code>
does this for you.
</p>
</div>
</div>
</div>

<div id="outline-container-org27ab30b" class="outline-2">
<h2 id="org27ab30b"><span class="section-number-2">4</span> Regularisation</h2>
<div class="outline-text-2" id="text-4">
<p>
Applying regularisation to model parameters is straightforward. We
just need to apply an appropriate regulariser, such as <code>vecnorm</code>, to
each model parameter and add the result to the overall loss.
</p>

<p>
For example, say we have a simple regression.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux: crossentropy
m = Dense(10, 5)
<span style="color: #d15120; background-color: #fdf2ed;">loss</span>(x, y) = crossentropy(softmax(m(x)), y)
</pre>
</div>

<pre class="example">

Dense(10, 5)
loss (generic function with 1 method)

</pre>

<p>
We can regularise this by taking the (<b>L2</b>) norm of the parameters,
<code>m.W</code> and <code>m.b</code>.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #d15120; background-color: #fdf2ed;">penalty</span>() = vecnorm(m.W) + vecnorm(m.b)
<span style="color: #d15120; background-color: #fdf2ed;">loss</span>(x, y) = crossentropy(softmax(m(x)), y) + penalty()
</pre>
</div>

<pre class="example">
penalty (generic function with 1 method)
loss (generic function with 1 method)

</pre>

<p>
When working with layers, Flux provides the <code>params</code> function to grab
all parameters at once. We can easily penalise everything with
<code>sum(vecnorm, params)</code>.
</p>

<div class="org-src-container">
<pre class="src src-julia">params(m)
</pre>
</div>

<pre class="example">
2-element Array{Any,1}:
 param([0.188884 -0.233068 … -0.459947 -0.397623; -0.265188 0.481616 … 0.0229318 0.588874; … ; -0.302913 0.219138 … -0.330004 0.133575; -0.353293 0.493455 … 0.498129 -0.244323])
 param([0.0, 0.0, 0.0, 0.0, 0.0])

</pre>

<div class="org-src-container">
<pre class="src src-julia">sum(vecnorm, params(m))
</pre>
</div>

<pre class="example">
2.589716546999882 (tracked)

</pre>

<p>
Here's a larger example with a multi-layer perceptron.
</p>

<div class="org-src-container">
<pre class="src src-julia">m = Chain(
  Dense(28^2, 128, relu),
  Dense(128, 32, relu),
  Dense(32, 10), softmax)

<span style="color: #d15120; background-color: #fdf2ed;">loss</span>(x, y) = crossentropy(m(x), y) + sum(vecnorm, params(m))

loss(rand(28^2), rand(10))
</pre>
</div>

<pre class="example">
Chain(Dense(784, 128, NNlib.relu), Dense(128, 32, NNlib.relu), Dense(32, 10), NNlib.softmax)

loss (generic function with 1 method)

39.73180248438863 (tracked)

</pre>

<p>
One can also easily add per-layer regularisation via the <code>activations</code>
function:
</p>

<div class="org-src-container">
<pre class="src src-julia">c = Chain(Dense(10,5,&#963;),Dense(5,2),softmax)
Flux.activations(c, rand(10))
sum(vecnorm, ans)
</pre>
</div>

<pre class="example">
Chain(Dense(10, 5, NNlib.σ), Dense(5, 2), NNlib.softmax)
3-element Array{Any,1}:
 param([0.33369, 0.471888, 0.407948, 0.426888, 0.560523])
 param([0.119035, -0.823319])
 param([0.719575, 0.280425])
2.6025991594948934 (tracked)

</pre>
</div>
</div>

<div id="outline-container-org45dbfac" class="outline-2">
<h2 id="org45dbfac"><span class="section-number-2">5</span> Model Reference</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org3ae605e" class="outline-3">
<h3 id="org3ae605e"><span class="section-number-3">5.1</span> Basic Layers</h3>
<div class="outline-text-3" id="text-5-1">
<p>
These core layers form the foundation of almost all neural networks.
</p>
</div>

<div id="outline-container-orgaa44ae3" class="outline-4">
<h4 id="orgaa44ae3"><span class="section-number-4">5.1.1</span> <code>Flux.Chain</code> — Type.</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
<code>Chain(layers...)</code>
</p>

<p>
Chain multiple layers / functions together, so that they are called in
sequence on a given input.
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">using</span> Flux
</pre>
</div>

<div class="org-src-container">
<pre class="src src-julia">m = Chain(x -&gt; x^2, x -&gt; x+1)
m(5) == 26

m = Chain(Dense(10, 5), Dense(5, 2))
x = rand(10)
m(x) == m[2](m[1](x))
</pre>
</div>

<pre class="example">
Chain(#1, #2)
true

Chain(Dense(10, 5), Dense(5, 2))
10-element Array{Float64,1}:
 0.877099
 0.406706
 0.157071
 0.961797
 0.708122
 0.451322
 0.755104
 0.0389636
 0.437714
 0.698057
true
</pre>

<div class="org-src-container">
<pre class="src src-julia">m(x)
m[2](m[1](x))
</pre>
</div>

<pre class="example">
Tracked 2-element Array{Float64,1}:
  0.279557
 -0.172473
Tracked 2-element Array{Float64,1}:
  0.279557
 -0.172473

</pre>

<p>
<code>Chain</code> also supports indexing and slicing, e.g. <code>m[2]</code> or
<code>m[1:end-1]</code>. <code>m[1:3](x)</code> will calculate the output of the first three
layers.
</p>

<p>
Chain function <a href="https://github.com/FluxML/Flux.jl/blob/ce88273880730990ef2e236b775b2080eca12f4a/src/layers/basic.jl#L1-L18">source</a>.
</p>
</div>
</div>

<div id="outline-container-org098adee" class="outline-4">
<h4 id="org098adee"><span class="section-number-4">5.1.2</span> <code>Flux.Dense</code> — Type.</h4>
<div class="outline-text-4" id="text-5-1-2">
<div class="org-src-container">
<pre class="src src-text">Dense(in::Integer, out::Integer, &#963; = identity)
</pre>
</div>

<p>
Creates a traditional Dense layer with parameters <code>W</code> and <code>b</code>.
</p>

<p>
<code>y = σ.(W * x .+ b)</code>
</p>

<p>
The input <code>x</code> must be a vector of length <code>in</code>, or a batch of vectors
represented as an <code>in × N</code> matrix. The out y will be a vector or batch
of length <code>out</code>.
</p>

<div class="org-src-container">
<pre class="src src-julia">d = Dense(5, 2)
d(rand(5))
</pre>
</div>

<pre class="example">
Dense(5, 2)
Tracked 2-element Array{Float64,1}:
 -0.378698
  0.621726

</pre>

<p>
<a href="https://github.com/FluxML/Flux.jl/blob/ce88273880730990ef2e236b775b2080eca12f4a/src/layers/basic.jl#L46-L65">source</a>.
</p>
</div>
</div>

<div id="outline-container-orgfee36e5" class="outline-4">
<h4 id="orgfee36e5"><span class="section-number-4">5.1.3</span> Flux.Conv — Type.</h4>
<div class="outline-text-4" id="text-5-1-3">
<div class="org-src-container">
<pre class="src src-text">Conv(size, in=&gt;out)
Conv(size, in=&gt;out, relu)
</pre>
</div>

<p>
Standard convolutional layer. <code>size</code> should be a tuple like <code>(2,
2)</code>. in and <code>out</code> specify the number of input and output channels
respectively.
</p>

<p>
Data should be stored in WHCN order. In other words, a <code>100×100</code> RGB
image would be a <code>100×100×3</code> array, and a batch of 50 would be a
<code>100×100×3×50</code> array.
</p>

<p>
Takes the keyword arguments pad, stride and dilation.
source
</p>

<p>
<a href="http://fluxml.ai/Flux.jl/stable/models/regularisation.html">http://fluxml.ai/Flux.jl/stable/models/regularisation.html</a>
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 17/07/2018</p>
<p class="author">Author: Sergio-Feliciano Mendoza-Barrera</p>
<p class="date">Created: 2018-07-23 Mon 11:33</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
