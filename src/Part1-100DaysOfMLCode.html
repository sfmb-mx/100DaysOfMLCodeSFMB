<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-07-24 Tue 20:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>100 Days Of Machine Learning Code</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Sergio-Feliciano Mendoza-Barrera" />
<meta name="description" content="Deep Learning Specialization series course"
 />
<meta name="keywords" content="R, data science, emacs, ESS, org-mode, deep learning" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">100 Days Of Machine Learning Code</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgdcfad11">1. 100DaysOfMLCodeSFMB</a></li>
<li><a href="#org4e9c43e">2. Day 1 [Thu Jul  5 20:58:13 CDT 2018]</a>
<ul>
<li><a href="#org36a2f61">2.1. Siraj Raval Tweet</a></li>
<li><a href="#org2404755">2.2. Challenge</a></li>
<li><a href="#org119ca74">2.3. Following my own path using Julia, R and Python</a></li>
<li><a href="#orge449c53">2.4. Install Julia</a></li>
<li><a href="#org5eafc06">2.5. Install python</a></li>
<li><a href="#org05316fa">2.6. Install Emacs</a></li>
</ul>
</li>
<li><a href="#orgcdcccc6">3. Day 2 [Fri Jul  6 19:21:03 CDT 2018]</a>
<ul>
<li><a href="#orgb738143">3.1. Course description</a></li>
<li><a href="#org98ed6a3">3.2. Prerequisites</a></li>
<li><a href="#orgb3d6e07">3.3. Grading</a></li>
<li><a href="#org3d04da1">3.4. Problem Sets</a></li>
<li><a href="#org83d6725">3.5. Projects</a></li>
<li><a href="#org811baf6">3.6. Syllabus</a></li>
<li><a href="#orgea4f1a8">3.7. Class 1. Course at a Glance</a></li>
</ul>
</li>
<li><a href="#org57e2f67">4. Day 3 [Sat Jul  7 13:12:57 CDT 2018]</a></li>
<li><a href="#orgfcbb45c">5. Day 4 [Sun Jul  8 12:59:19 CDT 2018]</a></li>
<li><a href="#orge33dae5">6. Day 5, 6, 7 [Init Mon Jul  9 16:12:01 CDT 2018]</a>
<ul>
<li><a href="#org0ffb6e2">6.1. Math camp</a></li>
<li><a href="#orgf765a56">6.2. Functional and Operators (Matrices)</a></li>
<li><a href="#org3837a72">6.3. Probability Space</a></li>
</ul>
</li>
<li><a href="#org082e7d7">7. Day 6 (Pending)</a>
<ul>
<li><a href="#orgd596df7">7.1. 9.520/6.860, Class 02</a></li>
</ul>
</li>
<li><a href="#org5e26cea">8. References</a></li>
</ul>
</div>
</div>
<div class="ABSTRACT">
<p>
According to the challenge we are going to explore a project and maybe
the possibility to monetize a solution of an industry problem using
ML.
</p>

<p>
<a href="https://twitter.com/sirajraval/status/1014758160572141568">URL</a>
</p>

<p>
"Who’s ready to take the 100 days of ML code challenge? That means
coding machine learning for at least an hour everyday for the next 100
days. Pledge with the #100DaysOfMLCode hashtag, I’ll give the first
few winners a shoutout!"
</p>

</div>

<div id="outline-container-orgdcfad11" class="outline-2">
<h2 id="orgdcfad11"><span class="section-number-2">1</span> 100DaysOfMLCodeSFMB</h2>
<div class="outline-text-2" id="text-1">
<p>
After read this tweet can be a great way to master something, do not
know what is, yet.
</p>

<p>
In day 2 I decided to explore the MIT course <b>9.520/6.860: Statistical
Learning Theory and Applications</b> <a href="http://www.mit.edu/~9.520/fall17/">Fall 2017</a>.
</p>
</div>
</div>

<div id="outline-container-org4e9c43e" class="outline-2">
<h2 id="org4e9c43e"><span class="section-number-2">2</span> Day 1 [Thu Jul  5 20:58:13 CDT 2018]</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org36a2f61" class="outline-3">
<h3 id="org36a2f61"><span class="section-number-3">2.1</span> Siraj Raval Tweet</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Who’s ready to take the 100 days of ML code challenge? That means
coding machine learning for at least an hour everyday for the next 100
days. Pledge with the #100DaysOfMLCode hashtag, I’ll give the first
few winners a shoutout!
</p>

<p>
<a href="https://twitter.com/sirajraval/status/1014758160572141568">Link to Tweet</a>.
</p>
</div>
</div>

<div id="outline-container-org2404755" class="outline-3">
<h3 id="org2404755"><span class="section-number-3">2.2</span> Challenge</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Pick an industry that sounds exciting, find a problem they have, think
about how AI could be applied to that problem, locate a relevant
dataset, apply AI to the dataset, monetize the solution.
</p>
</div>
</div>

<div id="outline-container-org119ca74" class="outline-3">
<h3 id="org119ca74"><span class="section-number-3">2.3</span> Following my own path using Julia, R and Python</h3>
<div class="outline-text-3" id="text-2-3">
<p>
In that order, trying to go as far as I can using the first one
syntax.
</p>
</div>
</div>

<div id="outline-container-orge449c53" class="outline-3">
<h3 id="orge449c53"><span class="section-number-3">2.4</span> Install Julia</h3>
<div class="outline-text-3" id="text-2-4">
<div class="org-src-container">
<pre class="src src-text">   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.3 (2018-05-28 20:20 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu
</pre>
</div>
</div>
</div>

<div id="outline-container-org5eafc06" class="outline-3">
<h3 id="org5eafc06"><span class="section-number-3">2.5</span> Install python</h3>
<div class="outline-text-3" id="text-2-5">
<div class="org-src-container">
<pre class="src src-text">Python version ::
sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)
=================
OpenCV version ::
3.4.0
=================
Tensorflow version ::
1.4.1
</pre>
</div>
</div>
</div>

<div id="outline-container-org05316fa" class="outline-3">
<h3 id="org05316fa"><span class="section-number-3">2.6</span> Install Emacs</h3>
<div class="outline-text-3" id="text-2-6">
<p>
I am joking, Emacs is always installed ;-).
</p>
</div>
</div>
</div>

<div id="outline-container-orgcdcccc6" class="outline-2">
<h2 id="orgcdcccc6"><span class="section-number-2">3</span> Day 2 [Fri Jul  6 19:21:03 CDT 2018]</h2>
<div class="outline-text-2" id="text-3">
<p>
9.520/6.860: Statistical Learning Theory and Applications, Fall 2017
</p>

<p>
<b>Instructors</b>: <a href="http://cbcl.mit.edu/people/poggio/poggio-new.htm">Tomaso Poggio (TP)</a>, <a href="http://web.mit.edu/lrosasco/www/">Lorenzo Rosasco (LR)</a>, <a href="http://web.mit.edu/gevang/www/">Georgios
Evangelopoulos (GE)</a>
</p>
</div>

<div id="outline-container-orgb738143" class="outline-3">
<h3 id="orgb738143"><span class="section-number-3">3.1</span> Course description</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The course covers foundations and recent advances of Machine Learning
from the point of view of Statistical Learning and Regularization
Theory.
</p>

<p>
Understanding intelligence and how to replicate it in machines is
arguably one of the greatest problems in science. Learning, its
principles and computational implementations, is at the very core of
intelligence. During the last decade, for the first time, we have been
able to develop artificial intelligence systems that can solve complex
tasks, until recently the exclusive domain of biological organisms,
such as computer vision, speech recognition or natural language
understanding: cameras recognize faces, smart phones understand voice
commands, smart speakers/assistants answer questions and cars can see
and avoid obstacles.
</p>

<p>
The machine learning algorithms that are at the roots of these success
stories are trained with labeled examples rather than programmed to
solve a task. Among the approaches in modern machine learning, the
course focuses on regularization techniques, that provide a
theoretical foundation to high-dimensional supervised
learning. Besides classic approaches such as Support Vector Machines,
the course covers state of the art techniques using sparsity or data
geometry (aka manifold learning), a variety of algorithms for
supervised learning (batch and online), feature selection, structured
prediction, and multitask learning and principles for designing or
learning data representations. Concepts from optimization theory
useful for machine learning are covered in some detail (first order
methods, proximal/splitting techniques,&#x2026;).
</p>

<p>
The final part of the course will focus on deep learning networks. It
will introduce an emerging theory formalizing three key areas for the
rigorous characterization of deep learning: approximation theory &#x2013;
which functions can be represented efficiently?; optimization theory
&#x2013; how easy is it to minimize the training error?; and generalization
properties &#x2013; is classical learning theory sufficient for deep
learning? It will also outline a theory of hierarchical architectures
that aims to explain how to build machine that learn using cortex
principles and similar to how children learn: from few labeled and
many more unlabeled data.
</p>

<p>
The goal of the course is to provide students with the theoretical
knowledge and the basic intuitions needed to use and develop effective
machine learning solutions to challenging problems.
</p>
</div>
</div>

<div id="outline-container-org98ed6a3" class="outline-3">
<h3 id="org98ed6a3"><span class="section-number-3">3.2</span> Prerequisites</h3>
<div class="outline-text-3" id="text-3-2">
<p>
We will make extensive use of basic notions of calculus, linear
algebra and probability. The essentials are covered in class and in
the math camp material. We will introduce a few concepts in
functional/convex analysis and optimization. Note that this is an
advanced graduate course and some exposure on introductory Machine
Learning concepts or courses is expected. Students are also expected
to have basic familiarity with MATLAB/Octave.
</p>
</div>
</div>

<div id="outline-container-orgb3d6e07" class="outline-3">
<h3 id="orgb3d6e07"><span class="section-number-3">3.3</span> Grading</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Pset and project tentative dates: (<a href="https://docs.google.com/presentation/d/1xt7U25g0AmOrBFxprJyq78qD45g80bH9Y2TbUNokYTE/">slides</a>).
</p>
</div>
</div>

<div id="outline-container-org3d04da1" class="outline-3">
<h3 id="org3d04da1"><span class="section-number-3">3.4</span> Problem Sets</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Problem Set 1
Problem Set 2
Problem Set 3
Problem Set 4
</p>

<p>
Submission instructions: Follow the instructions included with the
problem set. Use the <a href="http://www.mit.edu/~9.520/fall17/slides/9.520_pset_template.zip">latex template</a> for the report (there is a maximum
page limit). Submit your report online through stellar.mit by the due
date/time and a printout in the first class after the due date.
</p>
</div>
</div>

<div id="outline-container-org83d6725" class="outline-3">
<h3 id="org83d6725"><span class="section-number-3">3.5</span> Projects</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Reports are 1-page, extended abstracts using <a href="http://www.mit.edu/~9.520/fall17/slides/9.520_project_report_template.zip">NIPS style files</a>.
</p>
</div>

<div id="outline-container-orgaa66d61" class="outline-4">
<h4 id="orgaa66d61"><span class="section-number-4">3.5.1</span> Projects archive</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
<a href="http://www.mit.edu/~9.520/wikiprojects.html">List of Wikipedia</a> entries, created or edited as part of projects
during previous course offerings.
</p>
</div>
</div>
</div>

<div id="outline-container-org811baf6" class="outline-3">
<h3 id="org811baf6"><span class="section-number-3">3.6</span> Syllabus</h3>
<div class="outline-text-3" id="text-3-6">
<p>
<a href="http://www.mit.edu/~9.520/fall17/#briefsyllabus">URL</a>.
</p>

<p>
Follow the link for each class to find a detailed description,
suggested readings, and class slides. Some of the later classes may be
subject to reordering or rescheduling.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Class</th>
<th scope="col" class="org-left">Date</th>
<th scope="col" class="org-left">Title</th>
<th scope="col" class="org-left">Instructor(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Class 01</td>
<td class="org-left">Wed Sep 06</td>
<td class="org-left"><a href="http://www.mit.edu/~9.520/fall17/Classes/course_glance.html">The Course at a Glance</a></td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 02</td>
<td class="org-left">Mon Sep 11</td>
<td class="org-left"><a href="http://www.mit.edu/~9.520/fall17/Classes/learning_problem.html">The Learning Problem and Regularization</a></td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 03</td>
<td class="org-left">Wed Sep 13</td>
<td class="org-left">Reproducing Kernel Hilbert Spaces</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 04</td>
<td class="org-left">Mon Sep 18</td>
<td class="org-left">Positive Definite Functions, Feature Maps and Mercer Theorem</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 05</td>
<td class="org-left">Wed Sep 20</td>
<td class="org-left">Tikhonov Regularization and the Representer Theorem</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 06</td>
<td class="org-left">Mon Sep 25</td>
<td class="org-left">Logistic Regression and Support Vector Machines</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 07</td>
<td class="org-left">Wed Sep 27</td>
<td class="org-left">Regularized Least Squares</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 08</td>
<td class="org-left">Mon Oct 02</td>
<td class="org-left">Iterative Regularization via Early Stopping</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 09</td>
<td class="org-left">Wed Oct 04</td>
<td class="org-left">Learning with Stochastic Gradients</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 10</td>
<td class="org-left">Wed Oct 11</td>
<td class="org-left">Large Scale Kernel Methods</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 11</td>
<td class="org-left">Mon Oct 16</td>
<td class="org-left">Sparsity Based Regularization</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 12</td>
<td class="org-left">Wed Oct 18</td>
<td class="org-left">Convex Relaxation and Proximal Gradient</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 13</td>
<td class="org-left">Mon Oct 23</td>
<td class="org-left">Structured Sparsity Regularization</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 14</td>
<td class="org-left">Wed Oct 25</td>
<td class="org-left">Multiple Kernel Learning</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 15</td>
<td class="org-left">Mon Oct 30</td>
<td class="org-left">Learning Theory</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 16</td>
<td class="org-left">Wed Nov 01</td>
<td class="org-left">Generalization Error and Stability</td>
<td class="org-left">LR</td>
</tr>

<tr>
<td class="org-left">Class 17</td>
<td class="org-left">Mon Nov 06</td>
<td class="org-left">Online Learning II</td>
<td class="org-left">Sasha Rakhlin</td>
</tr>

<tr>
<td class="org-left">Class 18</td>
<td class="org-left">Wed Nov 08</td>
<td class="org-left">Online Learning II</td>
<td class="org-left">Sasha Rakhlin</td>
</tr>

<tr>
<td class="org-left">Class 19</td>
<td class="org-left">Mon Nov 13</td>
<td class="org-left">Data Representation by Design</td>
<td class="org-left">GE</td>
</tr>

<tr>
<td class="org-left">Class 20</td>
<td class="org-left">Wed Nov 15</td>
<td class="org-left">Learning Data Representation: Dictionary Learning</td>
<td class="org-left">GE</td>
</tr>

<tr>
<td class="org-left">Class 21</td>
<td class="org-left">Mon Nov 20</td>
<td class="org-left">Learning Data Representation: Neural Networks</td>
<td class="org-left">GE</td>
</tr>

<tr>
<td class="org-left">Class 22</td>
<td class="org-left">Wed Nov 22</td>
<td class="org-left">Deep Learning Theory: Approximation</td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 23</td>
<td class="org-left">Mon Nov 27</td>
<td class="org-left">Deep Learning Theory: Optimization</td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 24</td>
<td class="org-left">Wed Nov 29</td>
<td class="org-left">Deep Learning Theory: Generalization</td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 25</td>
<td class="org-left">Mon Dec 04</td>
<td class="org-left">Learning Data Representation: Invariance and Selectivity</td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 26</td>
<td class="org-left">Wed Dec 06</td>
<td class="org-left">Deep Networks and Visual Cortex</td>
<td class="org-left">TP</td>
</tr>

<tr>
<td class="org-left">Class 27</td>
<td class="org-left">Mon Dec 11</td>
<td class="org-left">Poster presentations (2 sessions)</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgea4f1a8" class="outline-3">
<h3 id="orgea4f1a8"><span class="section-number-3">3.7</span> Class 1. Course at a Glance</h3>
<div class="outline-text-3" id="text-3-7">
</div>
<div id="outline-container-orgfb0db51" class="outline-4">
<h4 id="orgfb0db51"><span class="section-number-4">3.7.1</span> Description</h4>
<div class="outline-text-4" id="text-3-7-1">
<p>
We introduce and motivate the main theme of much of the course,
setting the problem of supervised learning from examples as the
ill-posed problem of approximating a multivariate function from sparse
data. We present an overview of the theoretical part of the course and
sketch the connection between classical Regularization Theory with its
RKHS-based algorithms and Learning Theory. We briefly describe several
different applications ranging from vision to computer graphics, to
finance and neuroscience. The last third of the course will be on data
representations for learning and deep learning. It will introduce
recent theoretical developments towards a) understanding why deep
learning works and b) a new phase in machine learning, beyond
classical supervised learning: how to learn in an unsupervised way
representations that significantly decrease the sample complexity of a
supervised learning.
</p>
</div>
</div>

<div id="outline-container-org7753b89" class="outline-4">
<h4 id="org7753b89"><span class="section-number-4">3.7.2</span> Slides</h4>
<div class="outline-text-4" id="text-3-7-2">
<ul class="org-ul">
<li>Slides for this lecture: <a href="../docs/class01_2017.pdf">PDF</a>.</li>
</ul>
</div>

<div id="outline-container-orgce6dbf3" class="outline-5">
<h5 id="orgce6dbf3"><span class="section-number-5">3.7.2.1</span> Youtube video class 2015.</h5>
<div class="outline-text-5" id="text-3-7-2-1">
<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/6AWZS4Ho2Z8" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/6AWZS4Ho2Z8">Link here</a>
</p>
</div>
</div>

<div id="outline-container-orgc7ae7d0" class="outline-5">
<h5 id="orgc7ae7d0"><span class="section-number-5">3.7.2.2</span> 2017 Course - <a href="https://cbmm.mit.edu">Center for Brains, Minds and Machines</a> (CBMM)</h5>
<div class="outline-text-5" id="text-3-7-2-2">
<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/Q5itLKscYTA" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/Q5itLKscYTA">Link here</a>
</p>
</div>
</div>
</div>

<div id="outline-container-org2441195" class="outline-4">
<h4 id="org2441195"><span class="section-number-4">3.7.3</span> Relevant Reading</h4>
<div class="outline-text-4" id="text-3-7-3">
<ul class="org-ul">
<li>Mnih et. al. (Deep Mind), <a href="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html">Human-level control through deep
reinforcement learning</a>, Nature 518, pp. 529-533, 2015.</li>
<li>Nature Insights, <a href="http://www.nature.com/nature/supplements/insights/machine-intelligence/index.html">Machine Intelligence</a> (with review article on Deep
Learning), Nature, Vol. 521 No. 7553, pp. 435-482, 2015.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org57e2f67" class="outline-2">
<h2 id="org57e2f67"><span class="section-number-2">4</span> Day 3 [Sat Jul  7 13:12:57 CDT 2018]</h2>
<div class="outline-text-2" id="text-4">
<ol class="org-ol">
<li>Class 1 video [14.51]</li>
<li>Slide [26]</li>
</ol>
</div>
</div>

<div id="outline-container-orgfcbb45c" class="outline-2">
<h2 id="orgfcbb45c"><span class="section-number-2">5</span> Day 4 [Sun Jul  8 12:59:19 CDT 2018]</h2>
<div class="outline-text-2" id="text-5">
<ol class="org-ol">
<li>Class 1 Done</li>
</ol>
</div>
</div>

<div id="outline-container-orge33dae5" class="outline-2">
<h2 id="orge33dae5"><span class="section-number-2">6</span> Day 5, 6, 7 [Init Mon Jul  9 16:12:01 CDT 2018]</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org0ffb6e2" class="outline-3">
<h3 id="org0ffb6e2"><span class="section-number-3">6.1</span> Math camp</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Math camp extra class, optional for those interested: Tue. 09/12, 4:00
pm - 5:30 pm, Singleton auditorium (46-3002).
</p>
</div>

<div id="outline-container-org948c1dc" class="outline-4">
<h4 id="org948c1dc"><span class="section-number-4">6.1.1</span> Description</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
We review the basic prerequisites for the course on functional
analysis, linear algebra, probability theory and concentration of
measure.
</p>
</div>
</div>

<div id="outline-container-org6b689f0" class="outline-4">
<h4 id="org6b689f0"><span class="section-number-4">6.1.2</span> Class Reference Material</h4>
<div class="outline-text-4" id="text-6-1-2">
</div>
<div id="outline-container-org8d6f5cc" class="outline-5">
<h5 id="org8d6f5cc"><span class="section-number-5">6.1.2.1</span> Youtube video</h5>
<div class="outline-text-5" id="text-6-1-2-1">
<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/AsogCoscZgE" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/AsogCoscZgE">Link here</a>
</p>
</div>
</div>

<div id="outline-container-orgaa6646d" class="outline-5">
<h5 id="orgaa6646d"><span class="section-number-5">6.1.2.2</span> Local video</h5>
<div class="outline-text-5" id="text-6-1-2-2">
<p>
<a href="../videos/MathCampFor9_520_6_860S-StatisticalLearningTheoryAndApplications.mp4">Video</a> file.
</p>
</div>
</div>

<div id="outline-container-org88d1200" class="outline-5">
<h5 id="org88d1200"><span class="section-number-5">6.1.2.3</span> Slides</h5>
<div class="outline-text-5" id="text-6-1-2-3">
<p>
Slides: <a href="../docs/mathcamp-slides.pdf">PDF</a>. <a href="http://www.mit.edu/~9.520/fall17/slides/mathcamp/mathcamp-slides.pdf">Original URL</a>.
Notes/Book appendix: <a href="../docs/mathcamp-fa-notes_book.pdf">PDF</a>. <a href="http://www.mit.edu/~9.520/fall17/slides/mathcamp/mathcamp-fa-notes_book.pdf">Original URL</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd6ec5e2" class="outline-4">
<h4 id="orgd6ec5e2"><span class="section-number-4">6.1.3</span> Some concept testing with data</h4>
<div class="outline-text-4" id="text-6-1-3">
<p>
We like \(\mathbb{R}^D\) because we can
</p>

<p>
<b>Addition</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">v = [1, 2, 3];
w = [4, 5, 6];
println(v + w)
</pre>
</div>

<pre class="example">


[5, 7, 9]

</pre>

<p>
<b>Multiply by numbers</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">println(3 * v)
</pre>
</div>

<pre class="example">
[3, 6, 9]

</pre>

<p>
<b>Scalar product</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">println(v)
println(w)
dot(vec(v), vec(w))
dot(v, w)
</pre>
</div>

<pre class="example">
[1, 2, 3]
[4, 5, 6]
32
32

</pre>

<p>
<b>Norm</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">sqrt(dot(vec(v'), vec(v)))
vecnorm(v)
norm(v)
</pre>
</div>

<pre class="example">
3.7416573867739413
3.7416573867739413
3.7416573867739413

</pre>

<p>
<b>Distances between vectors</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">vecnorm(v - w)
norm(v - w)
</pre>
</div>

<pre class="example">
5.196152422706632
5.196152422706632

</pre>

<p>
<b>RMS value</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">norm(v) / sqrt(length(v))
</pre>
</div>

<pre class="example">
2.160246899469287

</pre>

<p>
<b>Standard deviation</b>
</p>

<p>
<i>Important note</i>: Julia do not use this definition.
</p>

<div class="org-src-container">
<pre class="src src-julia">norm(v - mean(v))/sqrt(length(v))
</pre>
</div>

<pre class="example">
0.8164965809277261

</pre>

<p>
Julia's way:
</p>

<div class="org-src-container">
<pre class="src src-julia">std(v)
</pre>
</div>

<pre class="example">
1.0

</pre>

<p>
<b>Angle between two vectors</b>
</p>

<div class="org-src-container">
<pre class="src src-julia">acos(dot(v, w)/(norm(v) * norm(w)))
</pre>
</div>

<pre class="example">
0.2257261285527342

</pre>

<p>
This what we called "Euclidean" structure. We want to do the samething
with \(D = \infty\)
</p>
</div>
</div>

<div id="outline-container-org87312cd" class="outline-4">
<h4 id="org87312cd"><span class="section-number-4">6.1.4</span> Vector Space</h4>
<div class="outline-text-4" id="text-6-1-4">

<div id="orgcc2b52e" class="figure">
<p><img src="../graphs/vectorSpace_mathCamp.png" alt="vectorSpace_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Vector Space</p>
</div>

<p>
Example: \(\mathbb{R}^D\), space of polynomials, space of functions.
</p>
</div>
</div>

<div id="outline-container-orga79b50b" class="outline-4">
<h4 id="orga79b50b"><span class="section-number-4">6.1.5</span> Inner Product</h4>
<div class="outline-text-4" id="text-6-1-5">

<div id="org1530c22" class="figure">
<p><img src="../graphs/innerProduct_mathCamp.png" alt="innerProduct_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Inner Product</p>
</div>
</div>
</div>

<div id="outline-container-org3efb731" class="outline-4">
<h4 id="org3efb731"><span class="section-number-4">6.1.6</span> Cauchy-Schwarz inequality</h4>
<div class="outline-text-4" id="text-6-1-6">
<p>
\(\langle v, w \rangle \le \langle v, v \rangle^{\frac{1}{2}} \langle w, w \rangle^{\frac{1}{2}}\).
</p>

<div class="org-src-container">
<pre class="src src-julia">println(<span style="color: #5f9411; background-color: #eff8e9;">":: v and w inner product ::"</span>)
dot(v, w)
println(<span style="color: #5f9411; background-color: #eff8e9;">":: must be less or equal to ::"</span>)
sqrt(dot(v, v)) * sqrt(dot(w, w))
</pre>
</div>

<pre class="example">
:: v and w inner product ::
32
:: must be less or equal to ::
32.83291031876401

</pre>
</div>
</div>

<div id="outline-container-orgfb6f83e" class="outline-4">
<h4 id="orgfb6f83e"><span class="section-number-4">6.1.7</span> Norm</h4>
<div class="outline-text-4" id="text-6-1-7">
<p>
Can define norm from inner product:
</p>

<p>
\(||v|| = \langle v, v \rangle^{\frac{1}{2}}\)
</p>


<div id="org92953c2" class="figure">
<p><img src="../graphs/norm_mathCamp.png" alt="norm_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Norm definition</p>
</div>
</div>
</div>

<div id="outline-container-org50d8213" class="outline-4">
<h4 id="org50d8213"><span class="section-number-4">6.1.8</span> Metric</h4>
<div class="outline-text-4" id="text-6-1-8">

<div id="org632224d" class="figure">
<p><img src="../graphs/distance_mathCamp.png" alt="distance_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Distance</p>
</div>
</div>
</div>

<div id="outline-container-org7654441" class="outline-4">
<h4 id="org7654441"><span class="section-number-4">6.1.9</span> Basis</h4>
<div class="outline-text-4" id="text-6-1-9">

<div id="org0c0df13" class="figure">
<p><img src="../graphs/basis_mathCamp.png" alt="basis_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Basis</p>
</div>
</div>
</div>

<div id="outline-container-org44963c6" class="outline-4">
<h4 id="org44963c6"><span class="section-number-4">6.1.10</span> Hilbert Space, overview</h4>
<div class="outline-text-4" id="text-6-1-10">
<p>
Goal: to understand Hilbert spaces (complete inner product spaces) and
to make sense of the expression
</p>

<p>
\[f = \sum_{i = 1}^{\infty} \langle f, \phi_i \rangle \phi_i, \ f \in \mathcal{H}\]
</p>

<p>
Need to talk about
</p>

<ol class="org-ol">
<li>Cauchy sequence</li>
<li>Completeness</li>
<li>Density</li>
<li>Separability</li>
</ol>
</div>
</div>

<div id="outline-container-org841b996" class="outline-4">
<h4 id="org841b996"><span class="section-number-4">6.1.11</span> Cauchy sequence</h4>
<div class="outline-text-4" id="text-6-1-11">

<div id="org2b7452a" class="figure">
<p><img src="../graphs/cauchySequence_mathCamp.png" alt="cauchySequence_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 6: </span>Cauchy Sequence</p>
</div>

<p>
See definition and examples in the video:
</p>

<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/d_190jhAifI" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/d_190jhAifI">Link here</a>
</p>

<div class="org-src-container">
<pre class="src src-julia"><span style="color: #cf7900; background-color: #fdf9f2;">function</span> <span style="color: #d15120; background-color: #fdf2ed;">nonCauchy</span>(n<span style="color: #505050; background-color: #FFFFFF;">::</span><span style="color: #b23f1e; background-color: #fcf3f1;">Int64</span>)
    <span style="color: #cf7900; background-color: #fdf9f2;">for</span> i = 1:n
        print((1 + ((-1)^i)), <span style="color: #5f9411; background-color: #eff8e9;">", "</span>)
    <span style="color: #cf7900; background-color: #fdf9f2;">end</span>
<span style="color: #cf7900; background-color: #fdf9f2;">end</span>

n = 12;
nonCauchy(n)
</pre>
</div>

<pre class="example">
nonCauchy (generic function with 1 method)


0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2,

</pre>
</div>
</div>

<div id="outline-container-org730c3d4" class="outline-4">
<h4 id="org730c3d4"><span class="section-number-4">6.1.12</span> Completeness</h4>
<div class="outline-text-4" id="text-6-1-12">

<div id="org55c24ac" class="figure">
<p><img src="../graphs/completeness_mathCamp.png" alt="completeness_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 7: </span>Completeness</p>
</div>
</div>
</div>

<div id="outline-container-org18e74e1" class="outline-4">
<h4 id="org18e74e1"><span class="section-number-4">6.1.13</span> Hilbert Space</h4>
<div class="outline-text-4" id="text-6-1-13">

<div id="org6cb55a5" class="figure">
<p><img src="../graphs/hilbertSpace_mathCamp.png" alt="hilbertSpace_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 8: </span>Hilbert Space</p>
</div>
</div>
</div>

<div id="outline-container-org34fe473" class="outline-4">
<h4 id="org34fe473"><span class="section-number-4">6.1.14</span> Orthonormal Basis</h4>
<div class="outline-text-4" id="text-6-1-14">
<ul class="org-ul">
<li>A Hilbert space has a countable orthonormal basis if and only if it
is separable.</li>
<li><p>
Can write:
</p>

<p>
\[f = \sum_{i = 1}^{\infty} \langle f, \phi_i \rangle \phi_i, \ for \ all \ f \in \mathcal{H}\]
</p></li>
</ul>


<div id="org7b88d8b" class="figure">
<p><img src="../graphs/OrthonormalBasis_mathCamp.png" alt="OrthonormalBasis_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 9: </span>Orthonormal Basis</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf765a56" class="outline-3">
<h3 id="orgf765a56"><span class="section-number-3">6.2</span> Functional and Operators (Matrices)</h3>
<div class="outline-text-3" id="text-6-2">
</div>
<div id="outline-container-orgddfde70" class="outline-4">
<h4 id="orgddfde70"><span class="section-number-4">6.2.1</span> Maps</h4>
<div class="outline-text-4" id="text-6-2-1">

<div id="orgefe55c7" class="figure">
<p><img src="../graphs/Maps_mathCamp.png" alt="Maps_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 10: </span>Maps</p>
</div>
</div>
</div>

<div id="outline-container-orgd8d2b81" class="outline-4">
<h4 id="orgd8d2b81"><span class="section-number-4">6.2.2</span> Representation of Continuous Functionals</h4>
<div class="outline-text-4" id="text-6-2-2">

<div id="orgef409ed" class="figure">
<p><img src="../graphs/RepresentationOfContinuousFunctionals_mathCamp.png" alt="RepresentationOfContinuousFunctionals_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 11: </span>Representation of continuous functionals</p>
</div>
</div>
</div>

<div id="outline-container-org760b0d5" class="outline-4">
<h4 id="org760b0d5"><span class="section-number-4">6.2.3</span> Matrix</h4>
<div class="outline-text-4" id="text-6-2-3">

<div id="org96c1aa7" class="figure">
<p><img src="../graphs/matrix_mathCamp.png" alt="matrix_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 12: </span>Matrix</p>
</div>

<ul class="org-ul">
<li>\(A\) is symmetric if \(A^T = A\)</li>
</ul>
</div>
</div>

<div id="outline-container-org9716a67" class="outline-4">
<h4 id="org9716a67"><span class="section-number-4">6.2.4</span> Eigenvalues and Eigenvectors</h4>
<div class="outline-text-4" id="text-6-2-4">

<div id="org41ec2ba" class="figure">
<p><img src="../graphs/EigenvaluesAndEigenvectors_mathCamp.png" alt="EigenvaluesAndEigenvectors_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 13: </span>Eigenvalues and Eigenvectors</p>
</div>

<p>
Video introduction to eigenvalues and eigenvectors
</p>

<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/G4N8vJpf7hM" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/G4N8vJpf7hM">Link here</a>
</p>

<div class="org-src-container">
<pre class="src src-julia">A = [3 2; 3 -2]
eig(A)
</pre>
</div>

<pre class="example">
2×2 Array{Int64,2}:
 3   2
 3  -2
([4.0, -3.0], [0.894427 -0.316228; 0.447214 0.948683])

</pre>

<p>
<b>MIT lecture</b>
</p>

<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/DzqE7tj7eIM" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/DzqE7tj7eIM">Link here</a>
</p>

<div class="org-src-container">
<pre class="src src-julia">A = [5 1; 3 3]
eig(A)
</pre>
</div>

<pre class="example">
2×2 Array{Int64,2}:
 5  1
 3  3
([6.0, 2.0], [0.707107 -0.316228; 0.707107 0.948683])

</pre>

<div class="org-src-container">
<pre class="src src-julia">println(<span style="color: #5f9411; background-color: #eff8e9;">":: Eigenvectors ::"</span>)
x1 = [1; 1]
x2 = [1; -3]
println(<span style="color: #5f9411; background-color: #eff8e9;">":: =============== ::"</span>)

println(<span style="color: #5f9411; background-color: #eff8e9;">":: First member A x1 ::"</span>)
A * x1
println(<span style="color: #5f9411; background-color: #eff8e9;">":: Second member lambda1 x1 ::"</span>)
lambda1 = 6.0
lambda1 * x1

println(<span style="color: #5f9411; background-color: #eff8e9;">":: =============== ::"</span>)
println(<span style="color: #5f9411; background-color: #eff8e9;">":: First member A x2 ::"</span>)
A * x2

println(<span style="color: #5f9411; background-color: #eff8e9;">":: Second member lambda2 x2 ::"</span>)
lambda2 = 2.0
lambda2 * x2
</pre>
</div>

<pre class="example">
:: Eigenvectors ::
2-element Array{Int64,1}:
 1
 1
2-element Array{Int64,1}:
  1
 -3
:: =============== ::

:: First member A x1 ::
2-element Array{Int64,1}:
 6
 6
:: Second member lambda1 x1 ::
6.0
2-element Array{Float64,1}:
 6.0
 6.0

:: =============== ::
:: First member A x2 ::
2-element Array{Int64,1}:
  2
 -6

:: Second member lambda2 x2 ::
2.0
2-element Array{Float64,1}:
  2.0
 -6.0
</pre>

<div class="org-src-container">
<pre class="src src-julia">A2 = [1 5; 3 3]
eig(A2)
</pre>
</div>

<pre class="example">
2×2 Array{Int64,2}:
 1  5
 3  3
([-2.0, 6.0], [-0.857493 -0.707107; 0.514496 -0.707107])

</pre>

<div class="org-src-container">
<pre class="src src-julia">println(<span style="color: #5f9411; background-color: #eff8e9;">":: Eigenvectors ::"</span>)
x1 = [-0.857493; 0.514496]
x2 = [-0.707107; -0.707107]
println(<span style="color: #5f9411; background-color: #eff8e9;">":: =============== ::"</span>)

println(<span style="color: #5f9411; background-color: #eff8e9;">":: First member A x1 ::"</span>)
A2 * x1
println(<span style="color: #5f9411; background-color: #eff8e9;">":: Second member lambda1 x1 ::"</span>)
lambda1 = -2.0
lambda1 * x1
println(<span style="color: #5f9411; background-color: #eff8e9;">":: =============== ::"</span>)

println(<span style="color: #5f9411; background-color: #eff8e9;">":: First member A x2 ::"</span>)
A2 * x2
println(<span style="color: #5f9411; background-color: #eff8e9;">":: Second member lambda2 x2 ::"</span>)
lambda2 = 6.0
lambda2 * x2
</pre>
</div>

<pre class="example">
:: Eigenvectors ::
2-element Array{Float64,1}:
 -0.857493
  0.514496
2-element Array{Float64,1}:
 -0.707107
 -0.707107
:: =============== ::

:: First member A x1 ::
2-element Array{Float64,1}:
  1.71499
 -1.02899
:: Second member lambda1 x1 ::
-2.0
2-element Array{Float64,1}:
  1.71499
 -1.02899
:: =============== ::

:: First member A x2 ::
2-element Array{Float64,1}:
 -4.24264
 -4.24264
:: Second member lambda2 x2 ::
6.0
2-element Array{Float64,1}:
 -4.24264
 -4.24264
</pre>

<p>
The <b>Eigendecomposition</b> is very useful in Machine Learning
algorithms.
</p>
</div>
</div>

<div id="outline-container-org8265695" class="outline-4">
<h4 id="org8265695"><span class="section-number-4">6.2.5</span> Singular Value Decomposition</h4>
<div class="outline-text-4" id="text-6-2-5">

<div id="org0a1f070" class="figure">
<p><img src="../graphs/SingularValueDecomposition_mathCamp.png" alt="SingularValueDecomposition_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 14: </span>Singular Value Decomposition</p>
</div>
</div>
</div>

<div id="outline-container-org250203c" class="outline-4">
<h4 id="org250203c"><span class="section-number-4">6.2.6</span> Matrix Norm</h4>
<div class="outline-text-4" id="text-6-2-6">

<div id="orgf391007" class="figure">
<p><img src="../graphs/MatrixNorm_mathCamp.png" alt="MatrixNorm_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 15: </span>Matrix Norm</p>
</div>
</div>
</div>

<div id="outline-container-orge5f8cc5" class="outline-4">
<h4 id="orge5f8cc5"><span class="section-number-4">6.2.7</span> Positive Definite Matrix</h4>
<div class="outline-text-4" id="text-6-2-7">

<div id="orgf5ca065" class="figure">
<p><img src="../graphs/PositiveDefiniteMatrix_mathCamp.png" alt="PositiveDefiniteMatrix_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 16: </span>Positive Definite Matrix</p>
</div>

<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/ojUQk_GNQbQ" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/ojUQk_GNQbQ">Link here</a>
</p>
</div>
</div>

<div id="outline-container-org5e9c842" class="outline-4">
<h4 id="org5e9c842"><span class="section-number-4">6.2.8</span> Adjoint and Compactness</h4>
<div class="outline-text-4" id="text-6-2-8">

<div id="orgc29d829" class="figure">
<p><img src="../graphs/AdjointAndCompactness_mathCamp.png" alt="AdjointAndCompactness_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 17: </span>Adjoint and Compactness</p>
</div>
</div>
</div>

<div id="outline-container-org965e7f7" class="outline-4">
<h4 id="org965e7f7"><span class="section-number-4">6.2.9</span> Spectral Theorem for Compact Self-Adjoint Operator</h4>
<div class="outline-text-4" id="text-6-2-9">

<div id="org6919998" class="figure">
<p><img src="../graphs/Theorem4CompactSelf-AdjointOperator_mathCamp.png" alt="Theorem4CompactSelf-AdjointOperator_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 18: </span>Spectral Theorem for Compact Self-Adjoint Operator</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3837a72" class="outline-3">
<h3 id="org3837a72"><span class="section-number-3">6.3</span> Probability Space</h3>
<div class="outline-text-3" id="text-6-3">

<div id="org738ddd5" class="figure">
<p><img src="../graphs/ProbabilitySpace_mathCamp.png" alt="ProbabilitySpace_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 19: </span>Probability Space</p>
</div>

<p>
<b>Typo</b>: \(P(X) = \infty\) must be \(P(X) = 1\)
</p>
</div>

<div id="outline-container-org7b358de" class="outline-4">
<h4 id="org7b358de"><span class="section-number-4">6.3.1</span> Real Random Variables</h4>
<div class="outline-text-4" id="text-6-3-1">

<div id="orgd163471" class="figure">
<p><img src="../graphs/RealRandomVariables_mathCamp.png" alt="RealRandomVariables_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 20: </span>Real Random Variables</p>
</div>

<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/ZvWZKLaZVjo" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/ZvWZKLaZVjo">Link here</a>
</p>
</div>
</div>

<div id="outline-container-orgb6a08e6" class="outline-4">
<h4 id="orgb6a08e6"><span class="section-number-4">6.3.2</span> Convergence of Random Variables</h4>
<div class="outline-text-4" id="text-6-3-2">

<div id="org24562aa" class="figure">
<p><img src="../graphs/ConvergenceOfRandomVariables_mathCamp.png" alt="ConvergenceOfRandomVariables_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 21: </span>Convergence of Random Variables</p>
</div>
</div>
</div>

<div id="outline-container-org20c8f72" class="outline-4">
<h4 id="org20c8f72"><span class="section-number-4">6.3.3</span> Law of Large Numbers</h4>
<div class="outline-text-4" id="text-6-3-3">

<div id="orga78a17e" class="figure">
<p><img src="../graphs/LawOfLargeNumbers_mathCamp.png" alt="LawOfLargeNumbers_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 22: </span>Law of Large Numbers</p>
</div>
</div>
</div>

<div id="outline-container-org90c47d3" class="outline-4">
<h4 id="org90c47d3"><span class="section-number-4">6.3.4</span> Concentration Inequalities</h4>
<div class="outline-text-4" id="text-6-3-4">

<div id="org8e1eae6" class="figure">
<p><img src="../graphs/ConcentrationInequalities_mathCamp.png" alt="ConcentrationInequalities_mathCamp.png" />
</p>
<p><span class="figure-number">Figure 23: </span>Concentration Inequalities (ERRONEOUS)</p>
</div>
</div>

<div id="outline-container-org274f70c" class="outline-5">
<h5 id="org274f70c"><span class="section-number-5">6.3.4.1</span> Chebyshev's Inequality</h5>
<div class="outline-text-5" id="text-6-3-4-1">
<p>
\[P(|X - \mu| \ge \epsilon) \le \frac{Var(X)}{\epsilon^2}\]
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org082e7d7" class="outline-2">
<h2 id="org082e7d7"><span class="section-number-2">7</span> Day 6 (Pending)</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgd596df7" class="outline-3">
<h3 id="orgd596df7"><span class="section-number-3">7.1</span> 9.520/6.860, Class 02</h3>
<div class="outline-text-3" id="text-7-1">
</div>
<div id="outline-container-org44eec47" class="outline-4">
<h4 id="org44eec47"><span class="section-number-4">7.1.1</span> Description</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
We formalize the problem of learning from examples in the framework of
statistical learning theory and introduce key terms and concepts such
as loss functions, empirical and excess risk, generalization error and
consistency. We briefly describe foundational results and introduce
the concepts of hypothesis space and regularization.
</p>
</div>
</div>

<div id="outline-container-org46c8c9e" class="outline-4">
<h4 id="org46c8c9e"><span class="section-number-4">7.1.2</span> Class Reference Material</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
<b>L. Rosasco, T. Poggio, Machine Learning: a Regularization Approach,
MIT-9.520 Lectures Notes, Manuscript, Dec. 2017</b>.
</p>
</div>
</div>

<div id="outline-container-orgb8bc2c9" class="outline-4">
<h4 id="orgb8bc2c9"><span class="section-number-4">7.1.3</span> Chapter 1 - Statistical Learning Theory</h4>
<div class="outline-text-4" id="text-7-1-3">
<p>
Note: The course notes, in the form of the circulated book draft is
the reference material for this class. Related and older material can
be accessed through previous year offerings of the course.
</p>
</div>
</div>

<div id="outline-container-orga8f8d28" class="outline-4">
<h4 id="orga8f8d28"><span class="section-number-4">7.1.4</span> Further Reading</h4>
<div class="outline-text-4" id="text-7-1-4">
<ul class="org-ul">
<li>F. Cucker and S. Smale, <a href="http://www.mit.edu/~9.520/Papers/cuckersmale.pdf">On the mathematical foundations of learning</a>,
Bulletin of the American Mathematical Society, 2002.</li>
<li>T. Evgeniou, M. Pontil and T. Poggio, <a href="http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf">Regularization networks and
support vector machines</a>, Advances in Computational
Mathematics, 2000.</li>
<li>S. Villa, L. Rosasco and T. Poggio, <a href="http://arxiv.org/pdf/1303.5976v1.pdf">On learnability, complexity and
stability</a>, "Empirical Inference, Festschrift in Honor of Vladimir
N. Vapnik." Springer-Verlag, Chapter 7, 2013.</li>
<li>V. Vapnik, An overview of statistical learning theory, IEEE
Trans. on Neural Networks , 10(5), 1999.</li>
</ul>
</div>
</div>

<div id="outline-container-orge2cc0d1" class="outline-4">
<h4 id="orge2cc0d1"><span class="section-number-4">7.1.5</span> Video</h4>
<div class="outline-text-4" id="text-7-1-5">
<p>
<iframe width="640" height="480" src="https://www.youtube.com/embed/SFxypsvhhMQ" frameborder="0" allowfullscreen></iframe>
</p>

<p>
<a href="https://youtu.be/SFxypsvhhMQ">Link here</a>
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org5e26cea" class="outline-2">
<h2 id="org5e26cea"><span class="section-number-2">8</span> References</h2>
<div class="outline-text-2" id="text-8">
<ol class="org-ol">
<li><a href="../docs/MLNotes.pdf">Introductory Machine Learning Notes. Lorenzo Rosasco,
MIT, 2017</a>. <a href="http://lcsl.mit.edu/courses/ml/1718/MLNotes.pdf">Original URL</a>.</li>

<li>Gilbert Strang MIT <a href="http://www-math.mit.edu/~gs">web page</a>.</li>

<li><a href="https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook">Multivariate Calculus best book</a>.</li>
</ol>

<div class="org-src-container">
<pre class="src src-julia">println(<span style="color: #5f9411; background-color: #eff8e9;">":: Update! ::"</span>)
</pre>
</div>

<pre class="example">
:: Update! ::

</pre>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 05/07/2018</p>
<p class="author">Author: Sergio-Feliciano Mendoza-Barrera</p>
<p class="date">Created: 2018-07-24 Tue 20:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
