#+TITLE:         100 Days Of Machine Learning Code
#+AUTHOR:        Sergio-Feliciano Mendoza-Barrera
#+DRAWERS:       sfmb
#+EMAIL:         s.f.m@ieee.org
#+DATE:          05/07/2018
#+DESCRIPTION:   Deep Learning Specialization series course
#+KEYWORDS:      R, data science, emacs, ESS, org-mode, deep learning
#+LANGUAGE:      en
#+OPTIONS:       H:10 num:t toc:nil \n:nil @:t ::t |:t ^:{} -:t f:t *:t <:t d:HIDDEN
#+OPTIONS:       TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:       LaTeX:dvipng
#+INFOJS_OPT:    view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="dft.css"/>

#+LaTeX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [letterpaper, 9pt, onecolumn, twoside, journal, final]
#+LATEX_HEADER: \usepackage[USenglish]{babel}
#+LATEX_HEADER: \hyphenation{do-cu-ment}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{makeidx}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[ttdefault=true]{AnonymousPro}
#+LATEX_HEADER: \renewcommand*\familydefault{\ttdefault} %% Only if the base font of the document is to be typewriter style
#+LATEX_HEADER: \usepackage[libertine,bigdelims]{newtxmath}
#+LATEX_HEADER: \usepackage[cal=boondoxo,bb=boondox,frak=boondox]{mathalfa}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \useosf % change normal text to use proportional oldstyle figures

#+LATEX_HEADER: \markboth{100 Days Of Machine Learning Code}%
#+LATEX_HEADER: {Sergio-Feliciano Mendoza-Barrera}

#+LATEX_HEADER: \newcommand{\degC}{$^\circ$C{}}

#+STYLE: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

#+ATTR_HTML: width="500px"

# -*- mode: org; -*-
#+OPTIONS:   toc:2

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

#+BEGIN_ABSTRACT
According to the challenge we are going to explore a project and maybe
the possibility to monetize a solution of an industry problem using
ML.

[[https://twitter.com/sirajraval/status/1014758160572141568][URL]]

"Who’s ready to take the 100 days of ML code challenge? That means
coding machine learning for at least an hour everyday for the next 100
days. Pledge with the #100DaysOfMLCode hashtag, I’ll give the first
few winners a shoutout!"
#+END_ABSTRACT

* 100DaysOfMLCodeSFMB

After read this tweet can be a great way to master something, do not
know what is, yet.

In day 2 I decided to explore the MIT course *9.520/6.860: Statistical
Learning Theory and Applications* [[http://www.mit.edu/~9.520/fall17/][Fall 2017]].

* Day 1 [Thu Jul  5 20:58:13 CDT 2018]

** Siraj Raval Tweet

Who’s ready to take the 100 days of ML code challenge? That means
coding machine learning for at least an hour everyday for the next 100
days. Pledge with the #100DaysOfMLCode hashtag, I’ll give the first
few winners a shoutout!

[[https://twitter.com/sirajraval/status/1014758160572141568][Link to Tweet]].

** Challenge

Pick an industry that sounds exciting, find a problem they have, think
about how AI could be applied to that problem, locate a relevant
dataset, apply AI to the dataset, monetize the solution.

** Following my own path using Julia, R and Python

In that order, trying to go as far as I can using the first one
syntax.

** Install Julia

#+BEGIN_SRC text
   _       _ _(_)_     |  A fresh approach to technical computing
  (_)     | (_) (_)    |  Documentation: https://docs.julialang.org
   _ _   _| |_  __ _   |  Type "?help" for help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 0.6.3 (2018-05-28 20:20 UTC)
 _/ |\__'_|_|_|\__'_|  |  Official http://julialang.org/ release
|__/                   |  x86_64-pc-linux-gnu
#+END_SRC

** Install python

#+BEGIN_SRC text
  Python version ::
  sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)
  =================
  OpenCV version ::
  3.4.0
  =================
  Tensorflow version ::
  1.4.1
#+END_SRC

** Install Emacs

I am joking, Emacs is always installed ;-).

* Day 2 [Fri Jul  6 19:21:03 CDT 2018]

9.520/6.860: Statistical Learning Theory and Applications, Fall 2017

*Instructors*: [[http://cbcl.mit.edu/people/poggio/poggio-new.htm][Tomaso Poggio (TP)]], [[http://web.mit.edu/lrosasco/www/][Lorenzo Rosasco (LR)]], [[http://web.mit.edu/gevang/www/][Georgios
Evangelopoulos (GE)]]

** Course description

The course covers foundations and recent advances of Machine Learning
from the point of view of Statistical Learning and Regularization
Theory.

Understanding intelligence and how to replicate it in machines is
arguably one of the greatest problems in science. Learning, its
principles and computational implementations, is at the very core of
intelligence. During the last decade, for the first time, we have been
able to develop artificial intelligence systems that can solve complex
tasks, until recently the exclusive domain of biological organisms,
such as computer vision, speech recognition or natural language
understanding: cameras recognize faces, smart phones understand voice
commands, smart speakers/assistants answer questions and cars can see
and avoid obstacles.

The machine learning algorithms that are at the roots of these success
stories are trained with labeled examples rather than programmed to
solve a task. Among the approaches in modern machine learning, the
course focuses on regularization techniques, that provide a
theoretical foundation to high-dimensional supervised
learning. Besides classic approaches such as Support Vector Machines,
the course covers state of the art techniques using sparsity or data
geometry (aka manifold learning), a variety of algorithms for
supervised learning (batch and online), feature selection, structured
prediction, and multitask learning and principles for designing or
learning data representations. Concepts from optimization theory
useful for machine learning are covered in some detail (first order
methods, proximal/splitting techniques,...).

The final part of the course will focus on deep learning networks. It
will introduce an emerging theory formalizing three key areas for the
rigorous characterization of deep learning: approximation theory --
which functions can be represented efficiently?; optimization theory
-- how easy is it to minimize the training error?; and generalization
properties -- is classical learning theory sufficient for deep
learning? It will also outline a theory of hierarchical architectures
that aims to explain how to build machine that learn using cortex
principles and similar to how children learn: from few labeled and
many more unlabeled data.

The goal of the course is to provide students with the theoretical
knowledge and the basic intuitions needed to use and develop effective
machine learning solutions to challenging problems.

** Prerequisites

We will make extensive use of basic notions of calculus, linear
algebra and probability. The essentials are covered in class and in
the math camp material. We will introduce a few concepts in
functional/convex analysis and optimization. Note that this is an
advanced graduate course and some exposure on introductory Machine
Learning concepts or courses is expected. Students are also expected
to have basic familiarity with MATLAB/Octave.

** Grading

Pset and project tentative dates: ([[https://docs.google.com/presentation/d/1xt7U25g0AmOrBFxprJyq78qD45g80bH9Y2TbUNokYTE/][slides]]).

** Problem Sets

Problem Set 1
Problem Set 2
Problem Set 3
Problem Set 4

Submission instructions: Follow the instructions included with the
problem set. Use the [[http://www.mit.edu/~9.520/fall17/slides/9.520_pset_template.zip][latex template]] for the report (there is a maximum
page limit). Submit your report online through stellar.mit by the due
date/time and a printout in the first class after the due date.

** Projects

Reports are 1-page, extended abstracts using [[http://www.mit.edu/~9.520/fall17/slides/9.520_project_report_template.zip][NIPS style files]].

*** Projects archive

[[http://www.mit.edu/~9.520/wikiprojects.html][List of Wikipedia]] entries, created or edited as part of projects
during previous course offerings.

** Syllabus

[[http://www.mit.edu/~9.520/fall17/#briefsyllabus][URL]].

Follow the link for each class to find a detailed description,
suggested readings, and class slides. Some of the later classes may be
subject to reordering or rescheduling.

| Class    | Date       | Title                                                        | Instructor(s) |
|----------+------------+--------------------------------------------------------------+---------------|
| Class 01 | Wed Sep 06 | [[http://www.mit.edu/~9.520/fall17/Classes/course_glance.html][The Course at a Glance]]                                       | TP            |
| Class 02 | Mon Sep 11 | [[http://www.mit.edu/~9.520/fall17/Classes/learning_problem.html][The Learning Problem and Regularization]]                      | LR            |
| Class 03 | Wed Sep 13 | Reproducing Kernel Hilbert Spaces                            | LR            |
| Class 04 | Mon Sep 18 | Positive Definite Functions, Feature Maps and Mercer Theorem | LR            |
| Class 05 | Wed Sep 20 | Tikhonov Regularization and the Representer Theorem          | LR            |
| Class 06 | Mon Sep 25 | Logistic Regression and Support Vector Machines              | LR            |
| Class 07 | Wed Sep 27 | Regularized Least Squares                                    | LR            |
| Class 08 | Mon Oct 02 | Iterative Regularization via Early Stopping                  | LR            |
| Class 09 | Wed Oct 04 | Learning with Stochastic Gradients                           | LR            |
| Class 10 | Wed Oct 11 | Large Scale Kernel Methods                                   | LR            |
| Class 11 | Mon Oct 16 | Sparsity Based Regularization                                | LR            |
| Class 12 | Wed Oct 18 | Convex Relaxation and Proximal Gradient                      | LR            |
| Class 13 | Mon Oct 23 | Structured Sparsity Regularization                           | LR            |
| Class 14 | Wed Oct 25 | Multiple Kernel Learning                                     | LR            |
| Class 15 | Mon Oct 30 | Learning Theory                                              | LR            |
| Class 16 | Wed Nov 01 | Generalization Error and Stability                           | LR            |
| Class 17 | Mon Nov 06 | Online Learning II                                           | Sasha Rakhlin |
| Class 18 | Wed Nov 08 | Online Learning II                                           | Sasha Rakhlin |
| Class 19 | Mon Nov 13 | Data Representation by Design                                | GE            |
| Class 20 | Wed Nov 15 | Learning Data Representation: Dictionary Learning            | GE            |
| Class 21 | Mon Nov 20 | Learning Data Representation: Neural Networks                | GE            |
| Class 22 | Wed Nov 22 | Deep Learning Theory: Approximation                          | TP            |
| Class 23 | Mon Nov 27 | Deep Learning Theory: Optimization                           | TP            |
| Class 24 | Wed Nov 29 | Deep Learning Theory: Generalization                         | TP            |
| Class 25 | Mon Dec 04 | Learning Data Representation: Invariance and Selectivity     | TP            |
| Class 26 | Wed Dec 06 | Deep Networks and Visual Cortex                              | TP            |
| Class 27 | Mon Dec 11 | Poster presentations (2 sessions)                            |               |

** Class 1. Course at a Glance

*** Description

We introduce and motivate the main theme of much of the course,
setting the problem of supervised learning from examples as the
ill-posed problem of approximating a multivariate function from sparse
data. We present an overview of the theoretical part of the course and
sketch the connection between classical Regularization Theory with its
RKHS-based algorithms and Learning Theory. We briefly describe several
different applications ranging from vision to computer graphics, to
finance and neuroscience. The last third of the course will be on data
representations for learning and deep learning. It will introduce
recent theoretical developments towards a) understanding why deep
learning works and b) a new phase in machine learning, beyond
classical supervised learning: how to learn in an unsupervised way
representations that significantly decrease the sample complexity of a
supervised learning.

*** Slides

- Slides for this lecture: [[file:../docs/class01_2017.pdf][PDF]].

**** Youtube video class 2015.

[[yt:6AWZS4Ho2Z8]]

[[https://youtu.be/6AWZS4Ho2Z8][Link here]]

**** 2017 Course - [[https://cbmm.mit.edu][Center for Brains, Minds and Machines]] (CBMM)

[[yt:Q5itLKscYTA]]

[[https://youtu.be/Q5itLKscYTA][Link here]]

*** Relevant Reading

- Mnih et. al. (Deep Mind), [[http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html][Human-level control through deep
  reinforcement learning]], Nature 518, pp. 529-533, 2015.
- Nature Insights, [[http://www.nature.com/nature/supplements/insights/machine-intelligence/index.html][Machine Intelligence]] (with review article on Deep
  Learning), Nature, Vol. 521 No. 7553, pp. 435-482, 2015.

* Day 3 [Sat Jul  7 13:12:57 CDT 2018]

1. Class 1 video [14.51]
2. Slide [26]

* Day 4 [Sun Jul  8 12:59:19 CDT 2018]

1. Class 1 Done

* Day 5 [Mon Jul  9 16:12:01 CDT 2018]

** Math camp

Math camp extra class, optional for those interested: Tue. 09/12, 4:00
pm - 5:30 pm, Singleton auditorium (46-3002).

*** Description

We review the basic prerequisites for the course on functional
analysis, linear algebra, probability theory and concentration of
measure.

*** Class Reference Material

**** Youtube video

[[yt:AsogCoscZgE]]

[[https://youtu.be/AsogCoscZgE][Link here]]

**** Local video

[[file:../videos/MathCampFor9_520_6_860S-StatisticalLearningTheoryAndApplications.mp4][Video]] file.

**** Slides

Slides: [[file:../docs/mathcamp-slides.pdf][PDF]]. [[http://www.mit.edu/~9.520/fall17/slides/mathcamp/mathcamp-slides.pdf][Original URL]].
Notes/Book appendix: [[file:../docs/mathcamp-fa-notes_book.pdf][PDF]]. [[http://www.mit.edu/~9.520/fall17/slides/mathcamp/mathcamp-fa-notes_book.pdf][Original URL]].

*** Some concept testing with data

We like $\mathbb{R}^D$ because we can

*Addition*

#+begin_src julia :session :results output :exports all
  v = [1, 2, 3];
  w = [4, 5, 6];
  println(v + w)
#+end_src

#+RESULTS:
:
:
: [5, 7, 9]

*Multiply by numbers*

#+begin_src julia :session :results output :exports all
  println(3 * v)
#+end_src

#+RESULTS:
: [3, 6, 9]

*Scalar product*

#+begin_src julia :session :results output :exports all
  println(v)
  println(w)
  dot(vec(v), vec(w))
  dot(v, w)
#+end_src

#+RESULTS:
: [1, 2, 3]
: [4, 5, 6]
: 32
: 32

*Norm*

#+begin_src julia :session :results output :exports all
  sqrt(dot(vec(v'), vec(v)))
  vecnorm(v)
  norm(v)
#+end_src

#+RESULTS:
: 3.7416573867739413
: 3.7416573867739413
: 3.7416573867739413

*Distances between vectors*

#+begin_src julia :session :results output :exports all
  vecnorm(v - w)
  norm(v - w)
#+end_src

#+RESULTS:
: 5.196152422706632
: 5.196152422706632

*RMS value*

#+begin_src julia :session :results output :exports all
  norm(v) / sqrt(length(v))
#+end_src

#+RESULTS:
: 2.160246899469287

*Standard deviation*

/Important note/: Julia do not use this definition.

#+begin_src julia :session :results output :exports all
  norm(v - mean(v))/sqrt(length(v))
#+end_src

#+RESULTS:
: 0.8164965809277261

Julia's way:

#+begin_src julia :session :results output :exports all
  std(v)
#+end_src

#+RESULTS:
: 1.0

*Angle between two vectors*

#+begin_src julia :session :results output :exports all
  acos(dot(v, w)/(norm(v) * norm(w)))
#+end_src

#+RESULTS:
: 0.2257261285527342

This what we called "Euclidean" structure. We want to do the samething
with $D = \infty$

*** Cauchy-Schwarz inequality

$\langle v, w \rangle \le \langle v, v \rangle^{\frac{1}{2}} \langle w, w \rangle^{\frac{1}{2}}$.

#+begin_src julia :session :results output :exports all
  println(":: v and w inner product ::")
  dot(v, w)
  println(":: must be less or equal to ::")
  sqrt(dot(v, v)) * sqrt(dot(w, w))
#+end_src

#+RESULTS:
: :: v and w inner product ::
: 32
: :: must be less or equal to ::
: 32.83291031876401

* Day 6 (Pending)

** 9.520/6.860, Class 02

*** Description

We formalize the problem of learning from examples in the framework of
statistical learning theory and introduce key terms and concepts such
as loss functions, empirical and excess risk, generalization error and
consistency. We briefly describe foundational results and introduce
the concepts of hypothesis space and regularization.

*** Class Reference Material

*L. Rosasco, T. Poggio, Machine Learning: a Regularization Approach,
MIT-9.520 Lectures Notes, Manuscript, Dec. 2017*.

*** Chapter 1 - Statistical Learning Theory

Note: The course notes, in the form of the circulated book draft is
the reference material for this class. Related and older material can
be accessed through previous year offerings of the course.

*** Further Reading

- F. Cucker and S. Smale, [[http://www.mit.edu/~9.520/Papers/cuckersmale.pdf][On the mathematical foundations of learning]],
  Bulletin of the American Mathematical Society, 2002.
- T. Evgeniou, M. Pontil and T. Poggio, [[http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf][Regularization networks and
  support vector machines]], Advances in Computational
  Mathematics, 2000.
- S. Villa, L. Rosasco and T. Poggio, [[http://arxiv.org/pdf/1303.5976v1.pdf][On learnability, complexity and
  stability]], "Empirical Inference, Festschrift in Honor of Vladimir
  N. Vapnik." Springer-Verlag, Chapter 7, 2013.
- V. Vapnik, An overview of statistical learning theory, IEEE
  Trans. on Neural Networks , 10(5), 1999.

*** Video

[[yt:SFxypsvhhMQ]]

[[https://youtu.be/SFxypsvhhMQ][Link here]]

* References

1. [[file:../docs/MLNotes.pdf][Introductory Machine Learning Notes. Lorenzo Rosasco,
   MIT, 2017]]. [[http://lcsl.mit.edu/courses/ml/1718/MLNotes.pdf][Original URL]].
